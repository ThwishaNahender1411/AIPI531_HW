{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Github link:"
      ],
      "metadata": {
        "id": "rNKsEsCRzAAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdrF3V_0b1zx",
        "outputId": "fc40c45b-aaf5-445c-aa64-679f13d094f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJ_DIR = '/content/drive/MyDrive/thwisha/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle'   ## give your drive folder location\n",
        "# change current directory after mounting\n",
        "%cd $PROJ_DIR\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp4q3rGTb6j4",
        "outputId": "d138cdb6-30d3-42db-c5b9-8a721c1dd324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle\n",
            "data\t\t     preprocess_kaggle.py  report_SNQN.txt   SNQN_Features_new.py  split_data.py\n",
            "DQN_NS.py\t     __pycache__\t   SA2C_new.py\t     SNQN_Features.py\t   test.py\n",
            "NextItNetModules.py  replay_buffer.py\t   SA2C.py\t     SNQN_new.py\t   utility.py\n",
            "pop.py\t\t     report_SA2C.txt\t   SASRecModules.py  SNQN.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "C5J0KvKySQ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_category=pd.read_csv(\"/content/drive/MyDrive/AIPI Assignment-3/category_tree.csv\")\n",
        "df_events=pd.read_csv(\"/content/drive/MyDrive/AIPI Assignment-3/events.csv\")\n",
        "df_items1=pd.read_csv(\"/content/drive/MyDrive/AIPI Assignment-3/item_properties_part1.csv\")\n",
        "df_items2=pd.read_csv(\"/content/drive/MyDrive/AIPI Assignment-3/item_properties_part2.csv\")"
      ],
      "metadata": {
        "id": "JIlGxoGMRw6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of category dataset:\", df_category.shape)\n",
        "print(\"Shape of the events dataset:\",df_events.shape)\n",
        "print(\"Shape of the items properties part 1 dataset:\", df_items1.shape)\n",
        "print(\"Shape of the items properties part 2 dataset:\", df_items2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz_ViVAuRxFT",
        "outputId": "10d77b3c-b2c6-49b8-fa92-c9c8588b7087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of category dataset: (1669, 2)\n",
            "Shape of the events dataset: (2756101, 5)\n",
            "Shape of the items properties part 1 dataset: (10999999, 4)\n",
            "Shape of the items properties part 2 dataset: (9275903, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted_events=pd.read_pickle(\"/content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/data/sorted_events.df\")"
      ],
      "metadata": {
        "id": "X9p6uJcN917H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_items = pd.concat([df_items1, df_items2], axis=0)\n",
        "df_items.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lue06j34RxJI",
        "outputId": "8d9b5601-331d-4d3f-e843-602f627d2f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20275902, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "events_item = df_sorted_events.item_id.unique()"
      ],
      "metadata": {
        "id": "IB_wKHrG9n-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "event_item_features_df = df_items[(df_items.itemid.isin(events_item)) &\n",
        "                                   (df_items['property'] == 'categoryid')].drop(['property', 'timestamp'], axis=1).drop_duplicates()\n",
        "\n",
        "\n",
        "missing_items_df = pd.DataFrame(set(events_item) - set(event_item_features_df.itemid.unique()), columns=['itemid'])\n",
        "missing_items_df['value'] = np.nan\n",
        "\n",
        "\n",
        "event_item_features_df = pd.concat([event_item_features_df, missing_items_df], axis=0)\n",
        "event_item_features_df.columns = ['itemid', 'categoryid']\n",
        "event_item_features_df.categoryid = event_item_features_df.categoryid.astype(float)\n",
        "\n",
        "event_category_features_df = event_item_features_df.merge(df_category, on='categoryid', how='left').drop_duplicates()\n",
        "parent_category_ids = event_category_features_df.parentid.unique()\n"
      ],
      "metadata": {
        "id": "DiCUA9-r-hVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_category_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VryGnJN_Lpc",
        "outputId": "e4490ca9-30e2-415a-e0fc-3fe75472ba35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(282,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for item in events_item:\n",
        "  row = event_category_features_df[event_category_features_df.itemid == item]\n",
        "  if len(row) == 0:\n",
        "    categorical = parent = np.nan\n",
        "  else:\n",
        "    categorical = row.categoryid.tolist()\n",
        "    parent = row.parentid.tolist()\n",
        "\n",
        "  data.append({'itemid': item, 'categoryid': categorical, 'parentid':parent})\n",
        "\n",
        "item_features_ = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "oNb9xwmo_j_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding = []\n",
        "for idx in range(item_features_.shape[0]):\n",
        "  one_hot_encoding.append(np.isin(parent_category_ids,\n",
        "                                 item_features_.parentid[idx]).astype(int))\n",
        "\n",
        "one_hot_encoding = np.array(one_hot_encoding)\n",
        "one_hot_encoding = pd.DataFrame(one_hot_encoding, index = item_features_.itemid,\n",
        "             columns = parent_category_ids)"
      ],
      "metadata": {
        "id": "pakL32OOASgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLbDkvqSBVbS",
        "outputId": "9a7d69cd-f930-42ec-ed59-69441b21972e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70852, 282)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding.to_csv('/content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/data/item_features.csv')"
      ],
      "metadata": {
        "id": "KQnVP6vVBIvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile 'SA2C.py' \\\n",
        "  --outfile 'SA2C_new.py' \\\n",
        "  --reportfile report_SA2C.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAZdiivkd4OO",
        "outputId": "c9e83973-47b0-4d9b-e4c6-4e23d82090c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-24 18:48:24.479376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-24 18:48:24.479431: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-24 18:48:24.479470: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-24 18:48:24.487221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-24 18:48:25.769254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO line 74:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 77:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 79:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 80:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "WARNING line 84:29: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 87:46: Renamed 'tf.nn.dynamic_rnn' to 'tf.compat.v1.nn.dynamic_rnn'\n",
            "INFO line 88:20: Renamed 'tf.contrib.rnn.GRUCell' to 'tf.compat.v1.nn.rnn_cell.GRUCell'\n",
            "INFO line 95:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 105:25: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 105:25: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 108:40: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 111:31: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 111:31: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 122:33: Added keywords to args of function 'tf.nn.max_pool'\n",
            "INFO line 122:33: Renamed keyword argument for tf.nn.max_pool from value to input\n",
            "INFO line 122:33: Renamed 'tf.nn.max_pool' to 'tf.nn.max_pool2d'\n",
            "INFO line 135:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 135:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 137:36: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 139:27: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 139:27: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 150:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 150:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 151:41: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 156:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "WARNING line 165:36: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "WARNING line 180:26: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 185:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 187:27: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 195:25: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "ERROR line 217:26: Using member tf.contrib.layers.dense in deprecated module tf.contrib. tf.contrib.layers.dense cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "INFO line 221:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 223:34: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 225:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 226:37: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 228:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 229:28: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 231:36: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 232:44: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 238:33: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 274:24: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 277:24: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 283:17: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 284:47: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 286:45: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 384:4: Renamed 'tf.reset_default_graph' to 'tf.compat.v1.reset_default_graph'\n",
            "INFO line 400:9: Renamed 'tf.Session' to 'tf.compat.v1.Session'\n",
            "INFO line 402:17: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 1 files\n",
            "Detected 4 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "File: SA2C.py\n",
            "--------------------------------------------------------------------------------\n",
            "SA2C.py:84:29: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SA2C.py:165:36: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SA2C.py:180:26: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SA2C.py:217:26: ERROR: Using member tf.contrib.layers.dense in deprecated module tf.contrib. tf.contrib.layers.dense cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report_SA2C.txt'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas trfl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKc7_j9UeOEr",
        "outputId": "56837424-aa29-4ad7-a4d5-9b91ee08925b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: trfl in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile 'SNQN_Features.py' \\\n",
        "  --outfile 'SNQN_Features_new.py' \\\n",
        "  --reportfile report_SA2C.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KkTS2uqbue",
        "outputId": "71d5520e-2d90-4e26-8d6d-642b4f9cd842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-24 18:52:18.993646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-24 18:52:18.993704: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-24 18:52:18.993745: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-24 18:52:19.001357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-24 18:52:20.219156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE] [--intree INPUT_TREE]\n",
            "                     [--outtree OUTPUT_TREE] [--copyotherfiles COPY_OTHER_FILES] [--inplace]\n",
            "                     [--no_import_rename] [--no_upgrade_compat_v1_import]\n",
            "                     [--reportfile REPORT_FILENAME] [--mode {DEFAULT,SAFETY}] [--print_all]\n",
            "tf_upgrade_v2: error: unrecognized arguments: --reportfile report_SA2C.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile 'SNQN.py' \\\n",
        "  --outfile 'SNQN_new.py' \\\n",
        "  --reportfile report_SNQN.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQSokODZq1a-",
        "outputId": "cd8e1b86-ae52-4d58-e940-8c188ba5edee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-24 18:57:16.471078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-24 18:57:16.471129: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-24 18:57:16.471170: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-24 18:57:16.478622: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-24 18:57:17.554509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO line 66:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 69:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 71:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 72:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "WARNING line 76:29: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 79:46: Renamed 'tf.nn.dynamic_rnn' to 'tf.compat.v1.nn.dynamic_rnn'\n",
            "INFO line 80:20: Renamed 'tf.contrib.rnn.GRUCell' to 'tf.compat.v1.nn.rnn_cell.GRUCell'\n",
            "INFO line 87:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 97:25: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 97:25: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 100:40: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 103:31: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 103:31: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 114:33: Added keywords to args of function 'tf.nn.max_pool'\n",
            "INFO line 114:33: Renamed keyword argument for tf.nn.max_pool from value to input\n",
            "INFO line 114:33: Renamed 'tf.nn.max_pool' to 'tf.nn.max_pool2d'\n",
            "INFO line 127:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 127:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 129:36: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 131:27: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 131:27: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 142:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 142:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 143:41: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 148:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "WARNING line 157:36: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "WARNING line 172:26: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 177:38: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 179:27: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 187:25: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "ERROR line 207:27: Using member tf.contrib.layers.fully_connected in deprecated module tf.contrib. tf.contrib.layers.fully_connected cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "ERROR line 210:26: Using member tf.contrib.layers.fully_connected in deprecated module tf.contrib. tf.contrib.layers.fully_connected cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "INFO line 214:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 216:34: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 218:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 219:37: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 221:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 222:28: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 224:36: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 225:44: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 244:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 249:17: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 250:47: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 252:45: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 337:4: Renamed 'tf.reset_default_graph' to 'tf.compat.v1.reset_default_graph'\n",
            "INFO line 348:9: Renamed 'tf.Session' to 'tf.compat.v1.Session'\n",
            "INFO line 350:17: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 1 files\n",
            "Detected 5 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "File: SNQN.py\n",
            "--------------------------------------------------------------------------------\n",
            "SNQN.py:76:29: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SNQN.py:157:36: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SNQN.py:172:26: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SNQN.py:207:27: ERROR: Using member tf.contrib.layers.fully_connected in deprecated module tf.contrib. tf.contrib.layers.fully_connected cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "SNQN.py:210:26: ERROR: Using member tf.contrib.layers.fully_connected in deprecated module tf.contrib. tf.contrib.layers.fully_connected cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report_SNQN.txt'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python SA2C_new.py --model=GRU --epoch=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pQT0DG6edr2",
        "outputId": "8be28264-e4b7-430f-f2de-27b07a5c0b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-15 21:37:02.315218: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 21:37:02.315268: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 21:37:02.315307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 21:37:02.322789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-15 21:37:03.437196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:88: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  tf.compat.v1.nn.rnn_cell.GRUCell(self.hidden_size),\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:87: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:585: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:599: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2023-11-15 21:37:05.451784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:05.483639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:05.483940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:05.485289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:05.485524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:05.485732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:06.227875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:06.228139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:06.228280: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-11-15 21:37:06.228363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:06.228510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:215: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num, activation=None)  # all q-values\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:217: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "2023-11-15 21:37:15.561969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:15.562289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:15.562465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:15.562730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:15.562951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-15 21:37:15.563086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-11-15 21:37:15.607995: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "the loss in 200th batch is: 10.836301\n",
            "the loss in 400th batch is: 10.624006\n",
            "the loss in 600th batch is: 10.251327\n",
            "the loss in 800th batch is: 10.390609\n",
            "the loss in 1000th batch is: 10.271822\n",
            "the loss in 1200th batch is: 10.280333\n",
            "the loss in 1400th batch is: 10.029512\n",
            "the loss in 1600th batch is: 9.748771\n",
            "the loss in 1800th batch is: 9.784418\n",
            "the loss in 2000th batch is: 9.907632\n",
            "the loss in 2200th batch is: 9.665062\n",
            "the loss in 2400th batch is: 9.443885\n",
            "the loss in 2600th batch is: 9.209308\n",
            "the loss in 2800th batch is: 9.255319\n",
            "the loss in 3000th batch is: 8.844220\n",
            "the loss in 3200th batch is: 8.954568\n",
            "the loss in 3400th batch is: 8.760042\n",
            "the loss in 3600th batch is: 8.446186\n",
            "the loss in 3800th batch is: 8.256122\n",
            "the loss in 4000th batch is: 8.411035\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 5949.800000\n",
            "clicks hr ndcg @ 5 : 0.169890, 0.134131\n",
            "purchase hr and ndcg @5 : 0.364770, 0.313405\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 6896.000000\n",
            "clicks hr ndcg @ 10 : 0.201385, 0.144334\n",
            "purchase hr and ndcg @10 : 0.402759, 0.325751\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 7408.800000\n",
            "clicks hr ndcg @ 15 : 0.219211, 0.149055\n",
            "purchase hr and ndcg @15 : 0.419958, 0.330341\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 7779.000000\n",
            "clicks hr ndcg @ 20 : 0.231476, 0.151955\n",
            "purchase hr and ndcg @20 : 0.435078, 0.333907\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.028911, 0.081144\n",
            "#############################################################\n",
            "the loss in 4200th batch is: 8.249550\n",
            "the loss in 4400th batch is: 8.448932\n",
            "the loss in 4600th batch is: 8.541065\n",
            "the loss in 4800th batch is: 8.221855\n",
            "the loss in 5000th batch is: 8.226561\n",
            "the loss in 5200th batch is: 7.979325\n",
            "the loss in 5400th batch is: 8.391852\n",
            "the loss in 5600th batch is: 7.703339\n",
            "the loss in 5800th batch is: 7.845350\n",
            "the loss in 6000th batch is: 7.678238\n",
            "the loss in 6200th batch is: 7.952054\n",
            "the loss in 6400th batch is: 7.813619\n",
            "the loss in 6600th batch is: 7.473821\n",
            "the loss in 6800th batch is: 7.150423\n",
            "the loss in 7000th batch is: 7.349873\n",
            "the loss in 7200th batch is: 7.413963\n",
            "the loss in 7400th batch is: 6.964683\n",
            "the loss in 7600th batch is: 7.346679\n",
            "the loss in 7800th batch is: 7.560416\n",
            "the loss in 8000th batch is: 7.046122\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8084.400000\n",
            "clicks hr ndcg @ 5 : 0.234113, 0.183809\n",
            "purchase hr and ndcg @5 : 0.481005, 0.414615\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9309.200000\n",
            "clicks hr ndcg @ 10 : 0.276495, 0.197549\n",
            "purchase hr and ndcg @10 : 0.522964, 0.428295\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 9981.200000\n",
            "clicks hr ndcg @ 15 : 0.299866, 0.203738\n",
            "purchase hr and ndcg @15 : 0.545455, 0.434255\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10424.400000\n",
            "clicks hr ndcg @ 20 : 0.315470, 0.207425\n",
            "purchase hr and ndcg @20 : 0.559441, 0.437552\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.056914, 0.149651\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 7.498579\n",
            "the loss in 8400th batch is: 7.348702\n",
            "the loss in 8600th batch is: 7.037866\n",
            "the loss in 8800th batch is: 7.384792\n",
            "the loss in 9000th batch is: 7.182468\n",
            "the loss in 9200th batch is: 6.979829\n",
            "the loss in 9400th batch is: 7.221091\n",
            "the loss in 9600th batch is: 6.542031\n",
            "the loss in 9800th batch is: 7.013913\n",
            "the loss in 10000th batch is: 7.065702\n",
            "the loss in 10200th batch is: 6.314329\n",
            "the loss in 10400th batch is: 6.755235\n",
            "the loss in 10600th batch is: 7.223490\n",
            "the loss in 10800th batch is: 6.926405\n",
            "the loss in 11000th batch is: 6.848601\n",
            "the loss in 11200th batch is: 6.829046\n",
            "the loss in 11400th batch is: 6.694181\n",
            "the loss in 11600th batch is: 6.421551\n",
            "the loss in 11800th batch is: 6.219464\n",
            "the loss in 12000th batch is: 6.597326\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8751.400000\n",
            "clicks hr ndcg @ 5 : 0.254907, 0.198781\n",
            "purchase hr and ndcg @5 : 0.514081, 0.436967\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 10133.800000\n",
            "clicks hr ndcg @ 10 : 0.301920, 0.214034\n",
            "purchase hr and ndcg @10 : 0.565111, 0.453696\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10859.400000\n",
            "clicks hr ndcg @ 15 : 0.327304, 0.220753\n",
            "purchase hr and ndcg @15 : 0.588736, 0.459963\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 11358.400000\n",
            "clicks hr ndcg @ 20 : 0.344928, 0.224921\n",
            "purchase hr and ndcg @20 : 0.604234, 0.463621\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.072005, 0.179160\n",
            "#############################################################\n",
            "the loss in 12200th batch is: 6.601942\n",
            "the loss in 12400th batch is: 6.448169\n",
            "the loss in 12600th batch is: 6.330000\n",
            "the loss in 12800th batch is: 6.448846\n",
            "the loss in 13000th batch is: 6.674769\n",
            "the loss in 13200th batch is: 6.027102\n",
            "the loss in 13400th batch is: 6.499873\n",
            "the loss in 13600th batch is: 6.409245\n",
            "the loss in 13800th batch is: 5.926412\n",
            "the loss in 14000th batch is: 6.510764\n",
            "the loss in 14200th batch is: 6.416117\n",
            "the loss in 14400th batch is: 5.940198\n",
            "the loss in 14600th batch is: 6.399539\n",
            "the loss in 14800th batch is: 6.466151\n",
            "the loss in 15000th batch is: 6.559353\n",
            "the loss in 15200th batch is: 1.345808\n",
            "the loss in 15400th batch is: 1.227286\n",
            "the loss in 15600th batch is: 1.154760\n",
            "the loss in 15800th batch is: 1.213583\n",
            "the loss in 16000th batch is: 1.050603\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8314.200000\n",
            "clicks hr ndcg @ 5 : 0.242304, 0.189758\n",
            "purchase hr and ndcg @5 : 0.487809, 0.413043\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9617.000000\n",
            "clicks hr ndcg @ 10 : 0.287940, 0.204525\n",
            "purchase hr and ndcg @10 : 0.529957, 0.426783\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10325.200000\n",
            "clicks hr ndcg @ 15 : 0.313517, 0.211296\n",
            "purchase hr and ndcg @15 : 0.549424, 0.431952\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10831.200000\n",
            "clicks hr ndcg @ 20 : 0.331226, 0.215480\n",
            "purchase hr and ndcg @20 : 0.565867, 0.435831\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.062206, 0.152382\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 0.988361\n",
            "the loss in 16400th batch is: 0.953315\n",
            "the loss in 16600th batch is: 0.968457\n",
            "the loss in 16800th batch is: 1.154483\n",
            "the loss in 17000th batch is: 1.035227\n",
            "the loss in 17200th batch is: 1.036737\n",
            "the loss in 17400th batch is: 1.051422\n",
            "the loss in 17600th batch is: 1.090028\n",
            "the loss in 17800th batch is: 1.003626\n",
            "the loss in 18000th batch is: 0.901289\n",
            "the loss in 18200th batch is: 0.882258\n",
            "the loss in 18400th batch is: 0.942953\n",
            "the loss in 18600th batch is: 0.936199\n",
            "the loss in 18800th batch is: 0.884224\n",
            "the loss in 19000th batch is: 0.871750\n",
            "the loss in 19200th batch is: 0.882356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python SA2C_new.py --model=Caser --epoch=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISYpw4RUfrFZ",
        "outputId": "f045cd5d-fadf-4fd2-b375-ab48f50d72fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 23:37:46.904971: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-16 23:37:46.905042: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-16 23:37:46.905083: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-16 23:37:46.912849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-16 23:37:48.049610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:151: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.states_hidden = tf.compat.v1.layers.dropout(self.final,\n",
            "2023-11-16 23:37:52.689663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:53.268200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:53.268519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:53.287677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:53.287949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:53.294234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:57.046491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:57.046900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:57.047154: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-11-16 23:37:57.047282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:37:57.047468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:215: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num, activation=None)  # all q-values\n",
            "/content/drive/MyDrive/AIPI Homework 3/SA2C_code/SA2C_code/Kaggle/SA2C_new.py:217: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "2023-11-16 23:38:12.588929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:38:12.589434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:38:12.589735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:38:12.590064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:38:12.590351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-16 23:38:12.590529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-11-16 23:38:12.652800: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "2023-11-16 23:38:17.154859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8906\n",
            "the loss in 200th batch is: 10.863541\n",
            "the loss in 400th batch is: 10.756878\n",
            "the loss in 600th batch is: 10.639463\n",
            "the loss in 800th batch is: 10.584887\n",
            "the loss in 1000th batch is: 10.380545\n",
            "the loss in 1200th batch is: 10.494829\n",
            "the loss in 1400th batch is: 10.478679\n",
            "the loss in 1600th batch is: 10.307783\n",
            "the loss in 1800th batch is: 10.197592\n",
            "the loss in 2000th batch is: 9.940256\n",
            "the loss in 2200th batch is: 10.039577\n",
            "the loss in 2400th batch is: 9.923860\n",
            "the loss in 2600th batch is: 10.030252\n",
            "the loss in 2800th batch is: 9.912477\n",
            "the loss in 3000th batch is: 9.824230\n",
            "the loss in 3200th batch is: 9.510586\n",
            "the loss in 3400th batch is: 9.525813\n",
            "the loss in 3600th batch is: 9.697691\n",
            "the loss in 3800th batch is: 9.606313\n",
            "the loss in 4000th batch is: 9.569230\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3152.600000\n",
            "clicks hr ndcg @ 5 : 0.091863, 0.073682\n",
            "purchase hr and ndcg @5 : 0.185031, 0.156743\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 3663.000000\n",
            "clicks hr ndcg @ 10 : 0.108490, 0.079038\n",
            "purchase hr and ndcg @10 : 0.207144, 0.163858\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 3983.800000\n",
            "clicks hr ndcg @ 15 : 0.118963, 0.081812\n",
            "purchase hr and ndcg @15 : 0.220941, 0.167516\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 4211.000000\n",
            "clicks hr ndcg @ 20 : 0.126325, 0.083548\n",
            "purchase hr and ndcg @20 : 0.230958, 0.169887\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.008901, 0.023151\n",
            "#############################################################\n",
            "the loss in 4200th batch is: 9.552413\n",
            "the loss in 4400th batch is: 9.583441\n",
            "the loss in 4600th batch is: 9.540228\n",
            "the loss in 4800th batch is: 9.304437\n",
            "the loss in 5000th batch is: 9.113552\n",
            "the loss in 5200th batch is: 9.391873\n",
            "the loss in 5400th batch is: 8.664082\n",
            "the loss in 5600th batch is: 9.431161\n",
            "the loss in 5800th batch is: 9.018910\n",
            "the loss in 6000th batch is: 8.803849\n",
            "the loss in 6200th batch is: 8.994468\n",
            "the loss in 6400th batch is: 8.663178\n",
            "the loss in 6600th batch is: 8.836215\n",
            "the loss in 6800th batch is: 8.819305\n",
            "the loss in 7000th batch is: 8.610668\n",
            "the loss in 7200th batch is: 8.480299\n",
            "the loss in 7400th batch is: 9.055421\n",
            "the loss in 7600th batch is: 8.852947\n",
            "the loss in 7800th batch is: 8.701476\n",
            "the loss in 8000th batch is: 8.633996\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 5199.000000\n",
            "clicks hr ndcg @ 5 : 0.153416, 0.123761\n",
            "purchase hr and ndcg @5 : 0.296541, 0.253638\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 6025.000000\n",
            "clicks hr ndcg @ 10 : 0.180380, 0.132481\n",
            "purchase hr and ndcg @10 : 0.332073, 0.265115\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 6491.400000\n",
            "clicks hr ndcg @ 15 : 0.196245, 0.136680\n",
            "purchase hr and ndcg @15 : 0.349272, 0.269681\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6811.600000\n",
            "clicks hr ndcg @ 20 : 0.206524, 0.139109\n",
            "purchase hr and ndcg @20 : 0.363825, 0.273118\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.024496, 0.061887\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 8.362362\n",
            "the loss in 8400th batch is: 8.484655\n",
            "the loss in 8600th batch is: 8.635127\n",
            "the loss in 8800th batch is: 8.140493\n",
            "the loss in 9000th batch is: 7.955996\n",
            "the loss in 9200th batch is: 8.845281\n",
            "the loss in 9400th batch is: 8.828166\n",
            "the loss in 9600th batch is: 8.389892\n",
            "the loss in 9800th batch is: 8.269986\n",
            "the loss in 10000th batch is: 8.401932\n",
            "the loss in 10200th batch is: 7.990956\n",
            "the loss in 10400th batch is: 8.232027\n",
            "the loss in 10600th batch is: 8.183906\n",
            "the loss in 10800th batch is: 8.252971\n",
            "the loss in 11000th batch is: 7.937855\n",
            "the loss in 11200th batch is: 8.011013\n",
            "the loss in 11400th batch is: 8.243260\n",
            "the loss in 11600th batch is: 8.179983\n",
            "the loss in 11800th batch is: 8.094022\n",
            "the loss in 12000th batch is: 7.671506\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 6166.200000\n",
            "clicks hr ndcg @ 5 : 0.183473, 0.147766\n",
            "purchase hr and ndcg @5 : 0.344925, 0.295586\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 7132.000000\n",
            "clicks hr ndcg @ 10 : 0.215416, 0.158124\n",
            "purchase hr and ndcg @10 : 0.384615, 0.308470\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 7663.200000\n",
            "clicks hr ndcg @ 15 : 0.233809, 0.163000\n",
            "purchase hr and ndcg @15 : 0.402759, 0.313278\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 8032.000000\n",
            "clicks hr ndcg @ 20 : 0.246099, 0.165902\n",
            "purchase hr and ndcg @20 : 0.417501, 0.316754\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.036764, 0.088122\n",
            "#############################################################\n",
            "the loss in 12200th batch is: 7.511830\n",
            "the loss in 12400th batch is: 7.690761\n",
            "the loss in 12600th batch is: 7.712177\n",
            "the loss in 12800th batch is: 7.565651\n",
            "the loss in 13000th batch is: 7.299454\n",
            "the loss in 13200th batch is: 7.507246\n",
            "the loss in 13400th batch is: 7.117473\n",
            "the loss in 13600th batch is: 7.512002\n",
            "the loss in 13800th batch is: 7.176417\n",
            "the loss in 14000th batch is: 7.256847\n",
            "the loss in 14200th batch is: 6.841794\n",
            "the loss in 14400th batch is: 7.361424\n",
            "the loss in 14600th batch is: 6.979273\n",
            "the loss in 14800th batch is: 7.121511\n",
            "the loss in 15000th batch is: 7.567645\n",
            "the loss in 15200th batch is: 0.663534\n",
            "the loss in 15400th batch is: 0.639391\n",
            "the loss in 15600th batch is: 0.669473\n",
            "the loss in 15800th batch is: 0.643730\n",
            "the loss in 16000th batch is: 0.616030\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 6359.600000\n",
            "clicks hr ndcg @ 5 : 0.190210, 0.152448\n",
            "purchase hr and ndcg @5 : 0.351351, 0.298636\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 7349.000000\n",
            "clicks hr ndcg @ 10 : 0.222728, 0.162953\n",
            "purchase hr and ndcg @10 : 0.392931, 0.312036\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 7901.400000\n",
            "clicks hr ndcg @ 15 : 0.241256, 0.167851\n",
            "purchase hr and ndcg @15 : 0.414477, 0.317733\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 8278.800000\n",
            "clicks hr ndcg @ 20 : 0.254332, 0.170939\n",
            "purchase hr and ndcg @20 : 0.427329, 0.320770\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.040113, 0.092393\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 0.726482\n",
            "the loss in 16400th batch is: 0.622188\n",
            "the loss in 16600th batch is: 0.668498\n",
            "the loss in 16800th batch is: 0.662691\n",
            "the loss in 17000th batch is: 0.669207\n",
            "the loss in 17200th batch is: 0.716663\n",
            "the loss in 17400th batch is: 0.627860\n",
            "the loss in 17600th batch is: 0.624411\n",
            "the loss in 17800th batch is: 0.620765\n",
            "the loss in 18000th batch is: 0.648473\n",
            "the loss in 18200th batch is: 0.580158\n",
            "the loss in 18400th batch is: 0.586568\n",
            "the loss in 18600th batch is: 0.566758\n",
            "the loss in 18800th batch is: 0.609563\n",
            "the loss in 19000th batch is: 0.655958\n",
            "the loss in 19200th batch is: 0.652125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python SNQN_new.py --model=GRU --epoch=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LokwKyudsDa6",
        "outputId": "215852fa-0542-4c7c-bd3b-43c3e8a542b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-24 19:11:56.820195: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-24 19:11:56.820256: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-24 19:11:56.820295: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-24 19:11:56.831476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-24 19:11:58.505952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_new.py:80: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  tf.compat.v1.nn.rnn_cell.GRUCell(self.hidden_size),\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_new.py:79: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:585: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:599: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2023-11-24 19:12:01.376955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:01.409461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:01.409740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:01.411074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:01.411307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:01.411492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:02.127251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:02.127527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:02.127665: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-11-24 19:12:02.127746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:02.127911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_new.py:207: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/drive/MyDrive/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_new.py:210: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "2023-11-24 19:12:09.593888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:09.594217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:09.594385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:09.594596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:09.594766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-24 19:12:09.594919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-11-24 19:12:09.620421: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 2.600000\n",
            "clicks hr ndcg @ 5 : 0.000025, 0.000017\n",
            "purchase hr and ndcg @5 : 0.000378, 0.000308\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4.600000\n",
            "clicks hr ndcg @ 10 : 0.000110, 0.000044\n",
            "purchase hr and ndcg @10 : 0.000378, 0.000308\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5.400000\n",
            "clicks hr ndcg @ 15 : 0.000144, 0.000053\n",
            "purchase hr and ndcg @15 : 0.000378, 0.000308\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 7.000000\n",
            "clicks hr ndcg @ 20 : 0.000211, 0.000069\n",
            "purchase hr and ndcg @20 : 0.000378, 0.000308\n",
            "#############################################################\n",
            "the loss in 200th batch is: 10.904570\n",
            "the loss in 400th batch is: 10.658080\n",
            "the loss in 600th batch is: 10.568659\n",
            "the loss in 800th batch is: 10.476092\n",
            "the loss in 1000th batch is: 10.380506\n",
            "the loss in 1200th batch is: 10.177391\n",
            "the loss in 1400th batch is: 10.059443\n",
            "the loss in 1600th batch is: 10.150692\n",
            "the loss in 1800th batch is: 9.683697\n",
            "the loss in 2000th batch is: 9.505902\n",
            "the loss in 2200th batch is: 9.779656\n",
            "the loss in 2400th batch is: 9.450330\n",
            "the loss in 2600th batch is: 9.228491\n",
            "the loss in 2800th batch is: 9.264203\n",
            "the loss in 3000th batch is: 9.052149\n",
            "the loss in 3200th batch is: 9.065473\n",
            "the loss in 3400th batch is: 8.774802\n",
            "the loss in 3600th batch is: 8.734557\n",
            "the loss in 3800th batch is: 8.827887\n",
            "the loss in 4000th batch is: 8.752385\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 5882.200000\n",
            "clicks hr ndcg @ 5 : 0.168766, 0.133752\n",
            "purchase hr and ndcg @5 : 0.357021, 0.302422\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 6809.800000\n",
            "clicks hr ndcg @ 10 : 0.197953, 0.143205\n",
            "purchase hr and ndcg @10 : 0.401814, 0.316996\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 7343.400000\n",
            "clicks hr ndcg @ 15 : 0.215517, 0.147858\n",
            "purchase hr and ndcg @15 : 0.424116, 0.322943\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 7700.200000\n",
            "clicks hr ndcg @ 20 : 0.227808, 0.150761\n",
            "purchase hr and ndcg @20 : 0.436590, 0.325879\n",
            "#############################################################\n",
            "the loss in 4200th batch is: 8.302271\n",
            "the loss in 4400th batch is: 8.299673\n",
            "the loss in 4600th batch is: 8.402706\n",
            "the loss in 4800th batch is: 7.828179\n",
            "the loss in 5000th batch is: 8.478937\n",
            "the loss in 5200th batch is: 8.193670\n",
            "the loss in 5400th batch is: 7.996597\n",
            "the loss in 5600th batch is: 7.929101\n",
            "the loss in 5800th batch is: 7.628252\n",
            "the loss in 6000th batch is: 8.121362\n",
            "the loss in 6200th batch is: 7.640425\n",
            "the loss in 6400th batch is: 7.869982\n",
            "the loss in 6600th batch is: 7.826610\n",
            "the loss in 6800th batch is: 7.577703\n",
            "the loss in 7000th batch is: 7.495737\n",
            "the loss in 7200th batch is: 7.229272\n",
            "the loss in 7400th batch is: 7.035228\n",
            "the loss in 7600th batch is: 6.940191\n",
            "the loss in 7800th batch is: 7.436587\n",
            "the loss in 8000th batch is: 7.246887\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8003.600000\n",
            "clicks hr ndcg @ 5 : 0.231628, 0.182798\n",
            "purchase hr and ndcg @5 : 0.476847, 0.408396\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9243.000000\n",
            "clicks hr ndcg @ 10 : 0.273528, 0.196392\n",
            "purchase hr and ndcg @10 : 0.523720, 0.423716\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 9883.200000\n",
            "clicks hr ndcg @ 15 : 0.296020, 0.202346\n",
            "purchase hr and ndcg @15 : 0.544132, 0.429158\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10357.000000\n",
            "clicks hr ndcg @ 20 : 0.312748, 0.206304\n",
            "purchase hr and ndcg @20 : 0.558874, 0.432646\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 6.903906\n",
            "the loss in 8400th batch is: 7.418735\n",
            "the loss in 8600th batch is: 7.399259\n",
            "the loss in 8800th batch is: 6.869610\n",
            "the loss in 9000th batch is: 7.243614\n",
            "the loss in 9200th batch is: 6.740849\n",
            "the loss in 9400th batch is: 7.136292\n",
            "the loss in 9600th batch is: 6.792235\n",
            "the loss in 9800th batch is: 6.782627\n",
            "the loss in 10000th batch is: 6.404018\n",
            "the loss in 10200th batch is: 6.665009\n",
            "the loss in 10400th batch is: 6.546097\n",
            "the loss in 10600th batch is: 6.415900\n",
            "the loss in 10800th batch is: 6.585336\n",
            "the loss in 11000th batch is: 6.576535\n",
            "the loss in 11200th batch is: 6.257208\n",
            "the loss in 11400th batch is: 6.596823\n",
            "the loss in 11600th batch is: 6.766888\n",
            "the loss in 11800th batch is: 6.525181\n",
            "the loss in 12000th batch is: 6.283210\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8724.000000\n",
            "clicks hr ndcg @ 5 : 0.253876, 0.198596\n",
            "purchase hr and ndcg @5 : 0.513514, 0.439608\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 10117.400000\n",
            "clicks hr ndcg @ 10 : 0.301988, 0.214202\n",
            "purchase hr and ndcg @10 : 0.561709, 0.455330\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10843.000000\n",
            "clicks hr ndcg @ 15 : 0.327794, 0.221039\n",
            "purchase hr and ndcg @15 : 0.583444, 0.461069\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 11327.800000\n",
            "clicks hr ndcg @ 20 : 0.344944, 0.225092\n",
            "purchase hr and ndcg @20 : 0.598375, 0.464584\n",
            "#############################################################\n",
            "the loss in 12200th batch is: 6.221139\n",
            "the loss in 12400th batch is: 6.394101\n",
            "the loss in 12600th batch is: 6.365522\n",
            "the loss in 12800th batch is: 6.112654\n",
            "the loss in 13000th batch is: 6.190166\n",
            "the loss in 13200th batch is: 6.183982\n",
            "the loss in 13400th batch is: 6.540998\n",
            "the loss in 13600th batch is: 6.021526\n",
            "the loss in 13800th batch is: 5.887138\n",
            "the loss in 14000th batch is: 6.097148\n",
            "the loss in 14200th batch is: 6.280560\n",
            "the loss in 14400th batch is: 6.306115\n",
            "the loss in 14600th batch is: 6.007039\n",
            "the loss in 14800th batch is: 6.393443\n",
            "the loss in 15000th batch is: 6.707154\n",
            "the loss in 15200th batch is: 6.079851\n",
            "the loss in 15400th batch is: 5.937516\n",
            "the loss in 15600th batch is: 5.806463\n",
            "the loss in 15800th batch is: 6.234283\n",
            "the loss in 16000th batch is: 5.879401\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 9008.000000\n",
            "clicks hr ndcg @ 5 : 0.263681, 0.205874\n",
            "purchase hr and ndcg @5 : 0.523342, 0.441255\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 10459.200000\n",
            "clicks hr ndcg @ 10 : 0.313053, 0.221868\n",
            "purchase hr and ndcg @10 : 0.576829, 0.458635\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 11210.800000\n",
            "clicks hr ndcg @ 15 : 0.340169, 0.229060\n",
            "purchase hr and ndcg @15 : 0.597619, 0.464157\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 11730.400000\n",
            "clicks hr ndcg @ 20 : 0.358367, 0.233361\n",
            "purchase hr and ndcg @20 : 0.614440, 0.468151\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 5.968896\n",
            "the loss in 16400th batch is: 5.908469\n",
            "the loss in 16600th batch is: 5.961205\n",
            "the loss in 16800th batch is: 6.115624\n",
            "the loss in 17000th batch is: 5.855148\n",
            "the loss in 17200th batch is: 5.839057\n",
            "the loss in 17400th batch is: 5.802312\n",
            "the loss in 17600th batch is: 5.862122\n",
            "the loss in 17800th batch is: 5.862966\n",
            "the loss in 18000th batch is: 6.022695\n",
            "the loss in 18200th batch is: 5.893702\n",
            "the loss in 18400th batch is: 5.740533\n",
            "the loss in 18600th batch is: 5.498778\n",
            "the loss in 18800th batch is: 5.559557\n",
            "the loss in 19000th batch is: 5.792739\n",
            "the loss in 19200th batch is: 6.222753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile 'SNQN_Features.py' \\\n",
        "  --outfile 'SNQN_Features_new.py' \\\n",
        "  --reportfile report_SA2C.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXAxlUluFFCG",
        "outputId": "0170464f-44fc-4b22-ba10-fb75c67a52e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-25 01:27:13.358756: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-25 01:27:13.358827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-25 01:27:13.358872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-25 01:27:13.371971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-25 01:27:14.789960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO line 102:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 107:13: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 109:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 112:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "WARNING line 117:29: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 122:46: Renamed 'tf.nn.dynamic_rnn' to 'tf.compat.v1.nn.dynamic_rnn'\n",
            "INFO line 123:20: Renamed 'tf.contrib.rnn.GRUCell' to 'tf.compat.v1.nn.rnn_cell.GRUCell'\n",
            "INFO line 131:20: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 142:25: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 142:25: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 146:28: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 150:31: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 150:31: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 162:33: Added keywords to args of function 'tf.nn.max_pool'\n",
            "INFO line 162:33: Renamed keyword argument for tf.nn.max_pool from value to input\n",
            "INFO line 162:33: Renamed 'tf.nn.max_pool' to 'tf.nn.max_pool2d'\n",
            "INFO line 178:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 178:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 181:24: Renamed 'tf.truncated_normal' to 'tf.random.truncated_normal'\n",
            "INFO line 184:27: Added keywords to args of function 'tf.nn.conv2d'\n",
            "INFO line 184:27: Renamed keyword argument for tf.nn.conv2d from filter to filters\n",
            "INFO line 198:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 198:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 199:41: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 206:20: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "WARNING line 223:36: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "WARNING line 244:26: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "INFO line 254:20: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 257:27: Renamed 'tf.layers.dropout' to 'tf.compat.v1.layers.dropout'\n",
            "INFO line 267:25: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 315:27: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 317:36: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 319:29: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 320:37: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 323:26: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 324:28: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 326:36: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 327:44: Renamed 'tf.placeholder' to 'tf.compat.v1.placeholder'\n",
            "INFO line 363:23: Renamed 'tf.train.AdamOptimizer' to 'tf.compat.v1.train.AdamOptimizer'\n",
            "INFO line 368:17: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\n",
            "INFO line 370:20: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 374:20: Renamed 'tf.random_normal' to 'tf.random.normal'\n",
            "INFO line 492:4: Renamed 'tf.reset_default_graph' to 'tf.compat.v1.reset_default_graph'\n",
            "INFO line 519:9: Renamed 'tf.Session' to 'tf.compat.v1.Session'\n",
            "INFO line 521:17: Renamed 'tf.global_variables_initializer' to 'tf.compat.v1.global_variables_initializer'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 1 files\n",
            "Detected 3 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "File: SNQN_Features.py\n",
            "--------------------------------------------------------------------------------\n",
            "SNQN_Features.py:117:29: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SNQN_Features.py:223:36: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "SNQN_Features.py:244:26: WARNING: tf.nn.embedding_lookup requires manual check. `partition_strategy` has been removed from tf.nn.embedding_lookup.  The 'div' strategy will be used by default.\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report_SA2C.txt'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python SNQN_Features_new.py --model=GRU --epoch=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxPk5O8WSPHX",
        "outputId": "e4c53e5e-8c08-411a-9b61-a8eb835bb867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-25 02:28:56.510605: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-25 02:28:56.510656: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-25 02:28:56.510694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-25 02:28:56.518709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-25 02:28:57.828588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_Features_new.py:123: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  tf.compat.v1.nn.rnn_cell.GRUCell(self.hidden_size),\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_Features_new.py:122: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:585: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/legacy_cells.py:599: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2023-11-25 02:29:04.448843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:05.028877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:05.029257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:05.031096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:05.031389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:05.031640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:08.370806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:08.371124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:08.371292: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-11-25 02:29:08.371384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:08.371568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_Features_new.py:292: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(\n",
            "/content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_Features_new.py:296: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2 = tf.compat.v1.layers.dense(\n",
            "/content/drive/.shortcut-targets-by-id/1mlkwkqJvfjZrMiy4ViJx-_vpAevMQXKU/AIPI Assignment-3/SA2C_code/SA2C_code/Kaggle/SNQN_Features_new.py:302: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.weights_fj = tf.compat.v1.layers.dense(\n",
            "2023-11-25 02:29:18.295604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:18.295943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:18.296125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:18.296336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:18.296494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-11-25 02:29:18.296615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13742 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2023-11-25 02:29:18.638256: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 1.200000\n",
            "clicks hr ndcg @ 5 : 0.000051, 0.000029\n",
            "purchase hr and ndcg @5 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 2.800000\n",
            "clicks hr ndcg @ 10 : 0.000118, 0.000051\n",
            "purchase hr and ndcg @10 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 4.800000\n",
            "clicks hr ndcg @ 15 : 0.000203, 0.000073\n",
            "purchase hr and ndcg @15 : 0.000000, 0.000000\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 5.400000\n",
            "clicks hr ndcg @ 20 : 0.000228, 0.000079\n",
            "purchase hr and ndcg @20 : 0.000000, 0.000000\n",
            "#############################################################\n",
            "the loss in 200th batch is: 10.921993\n",
            "the loss in 400th batch is: 10.668591\n",
            "the loss in 600th batch is: 10.431959\n",
            "the loss in 800th batch is: 10.352595\n",
            "the loss in 1000th batch is: 10.327320\n",
            "the loss in 1200th batch is: 10.193634\n",
            "the loss in 1400th batch is: 9.893464\n",
            "the loss in 1600th batch is: 10.190815\n",
            "the loss in 1800th batch is: 9.879097\n",
            "the loss in 2000th batch is: 9.734982\n",
            "the loss in 2200th batch is: 9.411195\n",
            "the loss in 2400th batch is: 9.589311\n",
            "the loss in 2600th batch is: 9.787023\n",
            "the loss in 2800th batch is: 9.335706\n",
            "the loss in 3000th batch is: 9.320900\n",
            "the loss in 3200th batch is: 8.875103\n",
            "the loss in 3400th batch is: 9.572367\n",
            "the loss in 3600th batch is: 8.896442\n",
            "the loss in 3800th batch is: 8.729367\n",
            "the loss in 4000th batch is: 8.458004\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 5237.400000\n",
            "clicks hr ndcg @ 5 : 0.148530, 0.115775\n",
            "purchase hr and ndcg @5 : 0.325647, 0.272239\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 6160.200000\n",
            "clicks hr ndcg @ 10 : 0.177979, 0.125298\n",
            "purchase hr and ndcg @10 : 0.368361, 0.286109\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 6659.800000\n",
            "clicks hr ndcg @ 15 : 0.195037, 0.129814\n",
            "purchase hr and ndcg @15 : 0.386505, 0.290917\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 7049.800000\n",
            "clicks hr ndcg @ 20 : 0.207969, 0.132867\n",
            "purchase hr and ndcg @20 : 0.402381, 0.294664\n",
            "#############################################################\n",
            "the loss in 4200th batch is: 9.045043\n",
            "the loss in 4400th batch is: 8.392782\n",
            "the loss in 4600th batch is: 9.010336\n",
            "the loss in 4800th batch is: 8.272966\n",
            "the loss in 5000th batch is: 8.113352\n",
            "the loss in 5200th batch is: 7.966378\n",
            "the loss in 5400th batch is: 7.939537\n",
            "the loss in 5600th batch is: 8.127114\n",
            "the loss in 5800th batch is: 8.063627\n",
            "the loss in 6000th batch is: 8.405749\n",
            "the loss in 6200th batch is: 7.507683\n",
            "the loss in 6400th batch is: 7.661556\n",
            "the loss in 6600th batch is: 7.761828\n",
            "the loss in 6800th batch is: 7.431125\n",
            "the loss in 7000th batch is: 7.858480\n",
            "the loss in 7200th batch is: 7.830517\n",
            "the loss in 7400th batch is: 7.580763\n",
            "the loss in 7600th batch is: 7.863723\n",
            "the loss in 7800th batch is: 7.103979\n",
            "the loss in 8000th batch is: 7.980085\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 7279.200000\n",
            "clicks hr ndcg @ 5 : 0.208916, 0.161848\n",
            "purchase hr and ndcg @5 : 0.441504, 0.368883\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 8569.800000\n",
            "clicks hr ndcg @ 10 : 0.251585, 0.175664\n",
            "purchase hr and ndcg @10 : 0.494613, 0.386150\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 9261.200000\n",
            "clicks hr ndcg @ 15 : 0.275819, 0.182078\n",
            "purchase hr and ndcg @15 : 0.516916, 0.392033\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 9733.400000\n",
            "clicks hr ndcg @ 20 : 0.291972, 0.185896\n",
            "purchase hr and ndcg @20 : 0.533926, 0.396055\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 7.121899\n",
            "the loss in 8400th batch is: 7.565275\n",
            "the loss in 8600th batch is: 7.743878\n",
            "the loss in 8800th batch is: 7.178214\n",
            "the loss in 9000th batch is: 6.700680\n",
            "the loss in 9200th batch is: 7.348040\n",
            "the loss in 9400th batch is: 7.192839\n",
            "the loss in 9600th batch is: 7.477833\n",
            "the loss in 9800th batch is: 6.602449\n",
            "the loss in 10000th batch is: 7.307815\n",
            "the loss in 10200th batch is: 7.261905\n",
            "the loss in 10400th batch is: 6.722502\n",
            "the loss in 10600th batch is: 6.729330\n",
            "the loss in 10800th batch is: 6.586061\n",
            "the loss in 11000th batch is: 6.754999\n",
            "the loss in 11200th batch is: 6.976777\n",
            "the loss in 11400th batch is: 6.917896\n",
            "the loss in 11600th batch is: 6.403205\n",
            "the loss in 11800th batch is: 6.688394\n",
            "the loss in 12000th batch is: 6.567629\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 7871.000000\n",
            "clicks hr ndcg @ 5 : 0.228264, 0.175904\n",
            "purchase hr and ndcg @5 : 0.466830, 0.387977\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9349.800000\n",
            "clicks hr ndcg @ 10 : 0.277957, 0.191988\n",
            "purchase hr and ndcg @10 : 0.524098, 0.406642\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10135.000000\n",
            "clicks hr ndcg @ 15 : 0.305268, 0.199214\n",
            "purchase hr and ndcg @15 : 0.550369, 0.413620\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10683.800000\n",
            "clicks hr ndcg @ 20 : 0.323728, 0.203577\n",
            "purchase hr and ndcg @20 : 0.571537, 0.418625\n",
            "#############################################################\n",
            "the loss in 12200th batch is: 6.791513\n",
            "the loss in 12400th batch is: 6.365612\n",
            "the loss in 12600th batch is: 6.553122\n",
            "the loss in 12800th batch is: 6.207755\n",
            "the loss in 13000th batch is: 6.388387\n",
            "the loss in 13200th batch is: 6.603007\n",
            "the loss in 13400th batch is: 6.402440\n",
            "the loss in 13600th batch is: 6.190116\n",
            "the loss in 13800th batch is: 6.576396\n",
            "the loss in 14000th batch is: 6.290755\n",
            "the loss in 14200th batch is: 6.420452\n",
            "the loss in 14400th batch is: 6.349550\n",
            "the loss in 14600th batch is: 6.329706\n",
            "the loss in 14800th batch is: 6.062453\n",
            "the loss in 15000th batch is: 6.028741\n",
            "the loss in 15200th batch is: 6.448972\n",
            "the loss in 15400th batch is: 6.181504\n",
            "the loss in 15600th batch is: 6.967836\n",
            "the loss in 15800th batch is: 5.733973\n",
            "the loss in 16000th batch is: 5.908831\n",
            "#############################################################\n",
            "total clicks: 118306, total purchase:5291\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8222.600000\n",
            "clicks hr ndcg @ 5 : 0.238855, 0.182071\n",
            "purchase hr and ndcg @5 : 0.485919, 0.394543\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9699.800000\n",
            "clicks hr ndcg @ 10 : 0.289411, 0.198436\n",
            "purchase hr and ndcg @10 : 0.539029, 0.411798\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10553.400000\n",
            "clicks hr ndcg @ 15 : 0.318175, 0.206051\n",
            "purchase hr and ndcg @15 : 0.571726, 0.420399\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 11107.400000\n",
            "clicks hr ndcg @ 20 : 0.337658, 0.210655\n",
            "purchase hr and ndcg @20 : 0.589303, 0.424538\n",
            "#############################################################\n",
            "the loss in 16200th batch is: 6.515921\n",
            "the loss in 16400th batch is: 6.190562\n",
            "the loss in 16600th batch is: 6.100716\n",
            "the loss in 16800th batch is: 5.886479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "\n",
        "The above results show the key metrics at different top k recommendations (5, 10, 15 and 20). SNQN_Features_new.py have the item features included. The inclusion of item features has improved the performance this is seen when we compare the key metrics throughout the training process. In both the models that is SNQN_new.py (does not contain item features) and SNQN_Features_new.py(contains item features) we can see that the metrics improve as the number of top-k recommendations increase, we can also see that for both the models the loss reduces, since the loss is reducing we can say that the models are learning. Since the model with item features is performing better we can say that including item features is beneficial in improving the recommendation quality."
      ],
      "metadata": {
        "id": "hYoNhQP8rS1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Successfully trained and compared session base DRL recommenders with and without item features. The inclusion of item features will help in improving the recommendation performance for e commerce applications\n"
      ],
      "metadata": {
        "id": "NZ-sqSqKtPrL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YviVFcePS2Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cys9_I4DtNRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}